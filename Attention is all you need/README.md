# Abstract

ê¸°ì¡´ì˜ Sequence transduction model ë“¤ì€ encoder ì™€ decoderë¥¼ í¬í•¨í•˜ëŠ” ë³µì¡í•œ recurrent ë˜ëŠ” CNN êµ¬ì¡°ì— ê¸°ë°˜í•œë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” Attention mechanism ë§Œì„ ì‚¬ìš©í•œ Transformer êµ¬ì¡°ë¥¼ ì œì•ˆí•œë‹¤.

ë²ˆì—­ task ë“¤ì„ í†µí•´ ìš°ìˆ˜í•œ ë³‘ë ¬ ì²˜ë¦¬ ì„±ëŠ¥ê³¼ í•™ìŠµ ì‹œê°„ì˜ ë‹¨ì¶•ì„ ë³´ì¸ë‹¤. BLEU í‰ê°€ ë°©ë²•ì„ í†µí•´ ê¸°ì¡´ì˜ task ì— ëŒ€í•´ ìš°ìˆ˜í•œ ì„±ì ì„ ì…ì¦í–ˆë‹¤.

# Instroduction

RNN, LSTM, GRU ëŠ” sequence modeling, transduction problem ë“¤ì— ëŒ€í•´ ìë¦¬ì¡ì€ ëª¨ë¸ë“¤ì´ì§€ë§Œ ì´ì „ ê²°ê³¼ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ëŠ” ìˆœì°¨ì  íŠ¹ì„±ìœ¼ë¡œ ì¸í•´ ë³‘ë ¬ ì²˜ë¦¬ì— ì–´ë ¤ì›€ì´ ìˆë‹¤. ìµœê·¼ì˜ ì—°êµ¬ì—ì„œ conditional computationì— ëŒ€í•œ ì„±ëŠ¥ ê°œì„ ì´ ì´ë£¨ì–´ì¡Œì§€ë§Œ ì—¬ì „íˆ ë³¸ì§ˆì ì¸ sequential computation ë¬¸ì œëŠ” ì¡´ì¬í•œë‹¤.

ê¸°ì¡´ì˜ Attention mechanism ì€ RNN ê³¼ í•¨ê»˜ ì‚¬ìš©ë˜ì—ˆë‹¤. ê·¸ëŸ¬ë‚˜ Transformer ëŠ” recurrenceë¥¼ ì œê±°í•´ ë³‘ë ¬ì„±ì„ í–¥ìƒì‹œì¼°ë‹¤.

# Background

Self-attention ì€ ë‹¨ì¼ sequenceì— ëŒ€í•´ **ìœ„ì¹˜ì— ë”°ë¼ ë‹¤ë¥¸ attention ì„ ìˆ˜í–‰**í•˜ëŠ” ê²ƒìœ¼ë¡œ ìŠ¤ìŠ¤ë¡œì— ëŒ€í•´attention ì„ ìˆ˜í–‰í•˜ëŠ” ê²ƒì´ë‹¤.

Transformer ëŠ” RNN ë˜ëŠ” convolution ì„ ì‚¬ìš©í•˜ì§€ ì•Šê³  self-attention ìœ¼ë¡œ input ê³¼ output ì„ ê³„ì‚°í•˜ëŠ” ì²« transduction model ì´ë‹¤.

# Model Architecture

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/542b861c-36a8-4051-84e5-8804b6728dba/d85562bd-05fd-4d06-9cff-b7bdfaee3a06/Untitled.png)

Transformer ëŠ” attention ë§Œìœ¼ë¡œ Encoder ì™€ Decoder ë¥¼ ê°€ì§„ êµ¬ì¡°ì´ë‹¤.

## Encoder

EncoderëŠ” 6ê°œì˜ ë…ë¦½ì ì¸ layer (Encoder Block) ë“¤ë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©° ê° layerëŠ” 2ê°œì˜ sub-layer ë¥¼ ê°–ëŠ”ë‹¤.

ì²«ë²ˆì§¸ë¡œëŠ” multi-head self-attention, ë‘ ë²ˆì§¸ëŠ” position-wise fully feed-forward network ì´ë‹¤.

ë‘ sub-layer ê°„ì—ëŠ” residual connection ê³¼ normalization ì„ ìˆ˜í–‰í•œë‹¤. ì´ëŠ” Gradient vanishing/exploding ë¬¸ì œë¥¼ ì™„í™”í•˜ê³  layer ê°„ì˜ ì •ë³´ ì „ë‹¬ì„ ë•ëŠ”ë‹¤.

ë…¼ë¬¸ì—ì„œ ëª¨ë¸ì˜ ëª¨ë“  sub-layer ì˜ ì°¨ì› $d_{model}$ ì€ 512ì´ë‹¤.

## Decoder

Decoder ë˜í•œ 6ê°œì˜ ë…ë¦½ì ì¸ layer (Decoder Block) ë“¤ë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©° Encoder ì— ì¡´ì¬í•˜ëŠ” ë‘ sub-layer ë¿ ì•„ë‹ˆë¼ Encoder ì˜ output ê³¼ multi-head attention ì„ ìˆ˜í–‰í•˜ëŠ” sub-layer ê°€ ì¡´ì¬í•œë‹¤.

ë§ˆì°¬ê°€ì§€ë¡œ ì„¸ sub-layer ê°„ residual connection ê³¼ normalization ì„ ìˆ˜í–‰í•œë‹¤.

## Attention

Attention ì€ query ì™€ key-value ìŒì„ output ì— mapping í•˜ëŠ” ê²ƒìœ¼ë¡œ ë¬˜ì‚¬ë  ìˆ˜ ìˆë‹¤. ì´ë•Œ query, key, value, output ì€ ëª¨ë‘ vector ì˜ í˜•íƒœì´ë‹¤.

Output ì€ ê°ê°ì˜ weighted sum(ê°€ì¤‘ì¹˜) ë¡œ ê³„ì‚°ë˜ë©° ê° ê°’ì— í• ë‹¹ë˜ëŠ” weight ëŠ” query ì™€ keyì˜ compatibility function ì— ì˜í•´ ê³„ì‚°ëœë‹¤.

<aside>
ğŸ’¡ ì‰½ê²Œ ë§í•´ attention ì˜ ëª©í‘œëŠ” value ë¥¼ í†µí•´ **weight sum ì„ ê³„ì‚°**í•˜ëŠ” ê²ƒì´ë©° value ì˜ weight ëŠ” **query ì™€ key ì˜ ìœ ì‚¬ë„ë¥¼ í†µí•´ ê²°ì •**ëœë‹¤.

</aside>

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/542b861c-36a8-4051-84e5-8804b6728dba/45ddcf3b-525a-4ed7-badd-ff9f814cd5e2/Untitled.png)

Transformer êµ¬ì¡°ì—ì„œ ì‚¬ìš©í•˜ëŠ” Attention ì€ ìœ„ì™€ ê°™ì€ êµ¬ì¡°ë¥¼ ê°€ì§„ë‹¤. 

ìˆ˜ì‹ì— ìˆì–´ query ì™€ key ì˜ ì°¨ì›ì„ $d_k$, value ì˜ ì°¨ì›ì„ $d_v$ ë¼ê³  í•˜ì. ë…¼ë¬¸ì—ì„œëŠ” $d_k,d_v$ ëŠ” 64ë¡œ ì„¤ì •í–ˆë‹¤.

Scaled Dot-Product Attention ì€ ì¢Œì¸¡ ê·¸ë¦¼ê³¼ ê°™ì´ ê³„ì‚°ëœë‹¤. Attention mechanism ì—ì„œëŠ” query ì˜ set ì„ ë™ì‹œì— matrix ë¡œ ê³„ì‚°í•œë‹¤. key ì™€ value ë˜í•œ ë§ˆì°¬ê°€ì§€ë¡œ ê³„ì‚°ë˜ëŠ”ë° ìˆ˜ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

$$
Attention(Q,K,V) = softmax(\cfrac{QK^T}{\sqrt{d_k}})V
$$

query ì˜ set ì¸ Q ì™€ key ì˜ set ì¸ K ë¥¼ í–‰ë ¬ê³± ì—°ì‚°í•˜ê³  ì´ë¥¼ ì°¨ì›ì˜ ë£¨íŠ¸ë¥¼ ì”Œìš´ ê°’ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì¤€ë‹¤. ë‚˜ëˆ„ì–´ ì£¼ëŠ” ì´ìœ ëŠ” ì°¨ì›ì´ ì»¤ì¡Œì„ ê²½ìš° í–‰ë ¬ê³± ê°’ë“¤ì´ ì»¤ì§€ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•¨ì´ë‹¤. ê·¸ë¦¼ì—ì„œì˜ **Scale** ê³¼ì •ìœ¼ë¡œ í° ê°’ì— ëŒ€í•´ì„œëŠ” softmax í•¨ìˆ˜ì˜ output ì„ ì•ˆì •í™”í•˜ê³  ì‘ì€ ê°’ì— ëŒ€í•´ì„œëŠ” vanishing ë¬¸ì œë¥¼ ë°©ì§€í•˜ëŠ” ì—­í• ì„ í•œë‹¤.

ì´ëŸ° Attention êµ¬ì¡°ê°€ ì—¬ëŸ¿ ëª¨ì—¬ ë§Œë“¤ì–´ì§„ ê²ƒì´ ìš°ì¸¡ì˜ **Multi-Head Attention** ì´ë‹¤. 

Multi-Head Attention ì€ Q, K, V ì— ëŒ€í•´ Linear ê³¼ì •ì„ ê±°ì¹œ í›„ Scaled Dot-Product Attention ì„ ìˆ˜í–‰í•˜ê³  ì´ ê°’ë“¤ì„ ë‹¤ì‹œ concatenate, Linear ê³¼ì •ì„ ìˆ˜í–‰í•´ output ì„ ìƒì„±í•œë‹¤.

<aside>
ğŸ’¡ **ê·¸ë ‡ë‹¤ë©´ concatenate ê³¼ì •ì€ ì™œ í•„ìš”í• ê¹Œ?**

ê·¸ ì´ìœ ëŠ” ì°¨ì›ì„ head ìˆ˜ë¡œ ìª¼ê°œì–´ h ë²ˆ í•™ìŠµì„ ì§„í–‰í•˜ê¸° ë•Œë¬¸ì´ë‹¤. Multi-Head Attention ë¼ëŠ” ì´ë¦„ì´ ë¶™ì€ ê²ƒë„ attention ì„ ì—¬ëŸ¬ head ë¡œ ë‚˜ëˆ„ì–´ ìˆ˜í–‰í•˜ê¸° ë•Œë¬¸ì´ë‹¤.
attention ì„ í•œë²ˆì— ìˆ˜í–‰í•˜ì§€ ì•Šê³  ë‚˜ëˆ„ì–´ ìˆ˜í–‰í•˜ëŠ” ì´ìœ ëŠ” multi-head ê°€ ì„œë¡œ ë‹¤ë¥¸ **Representation Subspaces** ë¥¼ í•™ìŠµí•˜ì—¬ ë‹¤ì–‘í•œ ìœ í˜•ì˜ ì¢…ì†ì„±ì„ ê°€ì ¸ ë‹¤ì–‘í•œ ì •ë³´ë¥¼ ê²°í•©í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤. ë¬¸ì¥ ë²ˆì—­ task ë¥¼ ì˜ˆë¡œ ë“¤ë©´ í•œ ë¬¸ì¥ì— ëŒ€í•´ ë¬¸ì¥ íƒ€ì… êµ¬ë³„, ëª…ì‚¬, ê´€ê³„, ê°•ì¡° ë“± ë‹¤ì–‘í•œ ìœ í˜•ì— ëŒ€í•´ ì§‘ì¤‘í•  ìˆ˜ ìˆë‹¤. ë…¼ë¬¸ì—ì„œëŠ” 8ê°œì˜ head ë¥¼ êµ¬ì„±í–ˆë‹¤.

</aside>

Multi-Head Attention ì„ ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

$$
MultiHead(Q,K,V)=Concat(head_1,...,head_h)W^O \\ where head_i=Attention(QW^Q_i,KW^K_i,VW^V_i)
$$

## Attention Models

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/542b861c-36a8-4051-84e5-8804b6728dba/d85562bd-05fd-4d06-9cff-b7bdfaee3a06/Untitled.png)

Transformer ì—ì„œëŠ” 3ê°€ì§€ì˜ attention model ì´ ì‚¬ìš©ë˜ì—ˆë‹¤.

### Encoder-Decoder Attention

ì´ layer ì—ì„œëŠ” query ëŠ” ì´ì „ decoder layer(block) ì—ì„œ ë°›ê³  key, value ëŠ” encoder ì˜ output ì—ì„œ ë°›ëŠ”ë‹¤. ìœ„ ê·¸ë¦¼ì—ì„œ Decoder ì˜ 2ë²ˆì§¸ layer, Multi-Head Attention layer ì— í•´ë‹¹í•œë‹¤. ì´ layer ë¥¼ í†µí•´ Decoder ì˜ sequence ë“¤ì´ Encoder ì˜ ì–´ë–¤ sequence ì™€ ì—°ê´€ì„ ê°–ëŠ”ì§€ í•™ìŠµí•  ìˆ˜ ìˆë‹¤.

### Encoder Self-Attention

ê·¸ë¦¼ì—ì„œ Encoder ì˜ Multi-Head Attention layer ì— í•´ë‹¹í•œë‹¤. Encoder ì˜ self-attention ì€ query, key, value ëª¨ë‘ encoder ì—ì„œ ê°€ì ¸ì˜¤ë©° ì´ì „ì˜ encoder layer(block) ì˜ output ì„ input ìœ¼ë¡œ ë°›ì•„ ì´ì „ layer ì˜ ëª¨ë“  position ì— ëŒ€í•´ í•™ìŠµ ê°€ëŠ¥í•˜ë‹¤.

### Decoder Self-Attention

ê·¸ë¦¼ì—ì„œ Decoder ì˜ Masked Multi-Head Attention layer ì— í•´ë‹¹í•œë‹¤. Encoder ì˜ self-attention layer ì™€ ì „ë°˜ì ìœ¼ë¡œ ë¹„ìŠ·í•œ ê¸°ëŠ¥ì„ í•˜ì§€ë§Œ decoder ì—ì„œëŠ” sequence model ì˜ Auto-Regressive property ë³´ì¡´ì„ ìœ„í•´ ì´í›„ì— ë‚˜ì˜¬ ë‹¨ì–´ë¥¼ ì°¸ì¡°í•˜ì§€ ì•ŠëŠ”ë‹¤. 

ì˜ˆë¥¼ ë“¤ì–´ I love dog ë¼ëŠ” ë¬¸ì¥ì´ ìˆì„ ë•Œì˜ query ê°€ love ë¼ë©´ dog ì— ëŒ€í•œ key ê°’ì€ ì°¸ì¡°í•˜ì§€ ì•ŠëŠ” ê²ƒì´ë‹¤. layer ì´ë¦„ì˜ Masked ê°€ ë¶™ì€ ê²ƒë„ ì´ì²˜ëŸ¼ ì°¸ì¡°í•˜ì§€ ì•ŠëŠ” í† í°(ì•„ì§ ì£¼ì–´ì§€ì§€ ì•Šì€ ì¿¼ë¦¬)ì— ëŒ€í•´ ë§ˆìŠ¤í‚¹ì„ ì§„í–‰í•˜ê¸° ë•Œë¬¸ì´ë‹¤.

## Position-wise Feed-Forward Networks

Encoder ì™€ Decoder ì— ëª¨ë‘ í¬í•¨ëœ **fully-connected feed-forward layer** ì´ë‹¤. 2ê°œì˜ Linear layer ì‚¬ì´ì— RELU activation ì„ ì‚¬ìš©í•œ êµ¬ì¡°ë¡œ ë…¼ë¬¸ì—ì„œëŠ” input, output ì˜ ì°¨ì›ì€ ë™ì¼í•˜ê²Œ $d_{model}=512$ ë¡œ ë‘ì—ˆê³  Linear ì‚¬ì´ì˜ ì°¨ì›ì¸ $d_{ff}=2048$ ë¡œ ì„¤ì •í–ˆë‹¤.

ì´ layer ë¥¼ í†µí•´ ë¹„ì„ í˜• ë³€í™”ë¥¼ ê±°ì¹˜ë©´ì„œ position ì •ë³´ë¥¼ ê°•í™”í•˜ê³  íŠ¹ì§•ì„ ì¶”ì¶œí•´ sequence ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì²˜ë¦¬, í•™ìŠµì‹œí‚¬ ìˆ˜ ìˆë„ë¡ í•œë‹¤.

## Embeddings and Softmax

### Embeddings

ë‹¤ë¥¸ sequence transduction model ë“¤ê³¼ ìœ ì‚¬í•˜ê²Œ input token, output token ë“¤ì„ $d_{model}$ ì°¨ì›ì˜ vector ë¡œ ë³€í™˜í•˜ê¸° ìœ„í•´ í•™ìŠµëœ embedding ì„ ì‚¬ìš©í•œë‹¤. 

input embedding ê³¼ output embedding ì€ $d_{model}$ ì°¨ì›ì˜ embedding layer ë¥¼ ê±°ì¹˜ê³  ë‘ embedding ì€ linear transform ìœ¼ë¡œ ê°™ì€ weight matrix ë¥¼ ê³µìœ í•œë‹¤. 

weight ì—ëŠ” $\sqrt{d_{model}}$ ì„ ê³±í•œë‹¤.

### Softmax

Decoder ì˜ output ì¶œë ¥ ê³¼ì •ì—ì„œ linear ë¥¼ ê±°ì¹œ í›„ softmax ë¥¼ ì·¨í•´ ë‹¤ìŒ token ì˜ í™•ë¥ ê°’ì„ ê³„ì‚°í•œë‹¤.

## Positional Encoding

Transformer ëª¨ë¸ì´ recurrence ë‚˜ convolution ì„ í¬í•¨í•˜ì§€ ì•Šê¸°ì— sequence ì˜ ìˆœì„œì— ëŒ€í•œ ì •ë³´ë¥¼ ë§Œë“¤ê¸° ìœ„í•´ sequence ì—ì„œ token ì˜ ìƒëŒ€ì  ë˜ëŠ” ì ˆëŒ€ì  ìœ„ì¹˜ì— ëŒ€í•œ ì •ë³´ë¥¼ ì£¼ì…í•´ì•¼ í•œë‹¤. ì´ ê³¼ì •ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ positional encoding ì´ ì‚¬ìš©ëœë‹¤.

ë…¼ë¬¸ì—ì„œëŠ” sin ê³¡ì„ ì˜ ìƒëŒ€ì  ìœ„ì¹˜ì— ëŒ€í•œ ì •ë³´ë¥¼ embedding ê³¼ ë™ì¼í•œ ì°¨ì›ì— ìœ„ì¹˜ì‹œí‚¤ëŠ” ë°©ë²•ì„ ì‚¬ìš©í–ˆë‹¤. Embedding ì°¨ì›ì¸ $d_{model}$ ê³¼ ë™ì¼í•œ ì°¨ì›ì˜ sin ê·¸ë˜í”„ë¥¼ ë‹¨ìˆœíˆ ë”í•´ì£¼ë©´ ëœë‹¤. ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.

$$
PE_{(pos,2i)}=sin(pos/10000^{2i/d_{model}}) \\ or \space PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{model}})
$$

ì´ë•Œ pos ëŠ” position ì˜ ìœ„ì¹˜, i ëŠ” hidden dimension ì˜ ìœ„ì¹˜ì´ë‹¤.

# Reference

[https://lcyking.tistory.com/entry/ë…¼ë¬¸ë¦¬ë·°-Attention-is-All-you-needì˜-ì´í•´](https://lcyking.tistory.com/entry/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0-Attention-is-All-you-need%EC%9D%98-%EC%9D%B4%ED%95%B4)

https://velog.io/@tobigs-nlp/Attention-is-All-You-Need-Transformer

---

# êµ¬í˜„

<aside>
ğŸ’¡ **ë³¸ ì½”ë“œëŠ” [ë™ë¹ˆë‚˜, [ë”¥ëŸ¬ë‹ ê¸°ê³„ ë²ˆì—­] Transformer: Attention Is All You Need (ê¼¼ê¼¼í•œ ë”¥ëŸ¬ë‹ ë…¼ë¬¸ ë¦¬ë·°ì™€ ì½”ë“œ ì‹¤ìŠµ)](https://www.youtube.com/watch?v=AA621UofTUA&t=5s) ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìŒ**

</aside>

> **ì‹¤ìŠµ ì½”ë“œ :** https://colab.research.google.com/drive/1pgQW82jfs0Q3d_pMXq-Tg-TRdj2wprBP?usp=sharing
> 

## Multi-Head Attention Layer

```python
import torch.nn as nn

class MultiHeadAttention(nn.Module):
  def __init__(self, hidden_dim, n_heads, dropout_ratio, device):
    super().__init__()

    self.hidden_dim = hidden_dim # embedding ì°¨ì›
    self.n_heads = n_heads # self attentionì˜ ìˆ˜
    self.head_dim = hidden_dim // n_heads # ê° headì—ì„œì˜ embedding ì°¨ì›

    # q, k, v layer
    self.fc_qkv = nn.Linear(hidden_dim, hidden_dim) # Query ê°’ì— ì ìš©ë  FC ë ˆì´ì–´

    # attention output layer
    self.fc_o = nn.Linear(hidden_dim, hidden_dim)

    self.dropout = nn.Dropout(dropout_ratio)

  def forward(self, q_in, k_in, v_in, mask=None):
    batch_size = q_in.shape[0]

    def transform_qkv(x):
	    # inputì˜ ì°¨ì› ë³€ê²½í•˜ëŠ” í•¨ìˆ˜
      # [batch_size, qkv_len, hidden_dim] -> [batch_size, n_heads, qkv_len, head_dim]
      out = self.fc_qkv(x)
      out = out.view(batch_size, -1, self.n_heads, self.head_dim).permute(0,2,1,3)
      return out

    # Q, K, V : [batch_size, n_heads, qkv_len, head_dim]
    Q = transform_qkv(q_in)
    K = transform_qkv(k_in)
    V = transform_qkv(v_in)

    # calculate attention
    # Attention = softmax( (Q*K_transposed) / sqrt(d_k) ) * V
    scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)

    # attention : [batch_size, n_heads, query_len, key_len]
    attention = torch.matmul(Q, K.permute(0,1,3,2)) 
    attention = attention / scale
    attention = torch.softmax(attention, dim=-1)

    out = torch.matmul(attention, V) # out : [batch_size, n_heads, query_len, head_dim]
    out = out.permute(0,2,1,3).contiguous() # out : [batch_size, query_len, n_heads, head_dim]
    out = out.view(batch_size, -1, self.hidden_dim) # out : [batch_size, query_len, hidden_dim]
    
    out = self.fc_qkv(out) # nn.Linear(self.hidden_dim, self.hidden_dim)

    return out, attention # attention : graph ê·¸ë¦¬ê¸° ìœ„í•´ ë°˜í™˜, ë°˜í™˜í•˜ì§€ ì•Šì•„ë„ ë¨

```

## Position-Wise Feed Foward Layer

```python
class PositionWiseFF(nn.Module):
  def __init__(self, hidden_dim, ff_dim, dropout_ratio): # ff_dim : FF layerì˜ ìì²´ dimension
    super().__init__()

    self.fc1 = nn.Linear(hidden_dim, ff_dim)
    self.relu = nn.ReLU()
    self.fc2 = nn.Linear(ff_dim, hidden_dim)

  def forward(self, out):
    out = self.fc1(out)
    out = self.relu(out)
    out = self.fc2(out)

    return out
```

## Encoder

### Encoder Block(Layer)

```python
class EncoderBlock(nn.Module):
  def __init__(self, hidden_dim, n_heads, ff_dim, dropout_ratio, device):
    super().__init__()

    self.self_attention_layer = MultiHeadAttention(hidden_dim, n_heads, dropout_ratio, device)
    self.self_attention_norm = nn.LayerNorm(hidden_dim)
    self.position_wise_ff_layer = PositionWiseFF(hidden_dim, ff_dim, dropout_ratio)
    self.position_wise_ff_norm = nn.LayerNorm(hidden_dim)
    self.dropout = nn.Dropout(dropout_ratio)

  def forward(self, src, src_mask):
    # src: [batch_size, src_len, hidden_dim]
    # src_mask: [batch_size, src_len]
    
    # q,k,v = src, í•„ìš”ì‹œ src_mask ì‚¬ìš©
    _src, a = self.self_attention_layer(src, src, src, src_mask)
    
    # residual connection : self.dropout(_src) ë”í•¨
    src = self.self_attention_norm(src + self.dropout(_src)) # ì…ë ¥ src + attention
    _src = self.position_wise_ff_layer(src)
    src = self.position_wise_ff_norm(src + self.dropout(_src))

    return src # src: [batch_size, src_len, hidden_dim]
```

### Encoder

- Encoder Block ì´ n_layers ë§Œí¼ ì´ì–´ì§„ êµ¬ì¡°

```python
class Encoder(nn.Module):
  def __init__(self, input_dim, hidden_dim, n_heads, ff_dim, dropout_ratio, device, n_layers, max_len=100):
    super().__init__()

    self.device = device

    # embedding ë³„ë„ ì„ ì–¸ í›„ ì‚¬ìš©ë„ ê°€ëŠ¥
    self.tok_embedding = nn.Embedding(input_dim, hidden_dim)
    self.pos_embedding = nn.Embedding(max_len, hidden_dim)

    # n_layers ë§Œí¼ Encoder Block ë°˜ë³µ
    self.layers = nn.ModuleList([EncoderBlock(hidden_dim, n_heads, ff_dim, dropout_ratio, device) for i in range(n_layers)])

    self.dropout = nn.Dropout(dropout_ratio)
    self.scale = torch.sqrt(torch.FloatTensor([hidden_dim])).to(device)

  def forward(self, src, src_mask):
    # src: [batch_size, src_len]
    # src_mask: [batch_size, src_len]

    batch_size = src.shape[0] # batch ë‚´ì˜ ë¬¸ì¥ ìˆ˜
    src_len = src.shape[1] # batch ë‚´ ë¬¸ì¥ ì¤‘ ê°€ì¥ ê¸´ ê²ƒì˜ ê¸¸ì´

    '''
    Positional Embedding
      pos: [batch_size, src_len]
      torch.arrange(0, src_len) : [src_len]
      unsqueeze(0) : [1, src_len]
      repeat(batch_size, 1) : [batch_size, src_len]
    '''
    pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)

    # ì†ŒìŠ¤ ë¬¸ì¥ì˜ ì„ë² ë”©ê³¼ ìœ„ì¹˜ ì„ë² ë”©ì„ ë”í•œ ê²ƒì„ ì‚¬ìš©
    src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos)) # src: [batch_size, src_len, hidden_dim]
    
    for layer in self.layers:
      src = layer(src, src_mask)

    return src
```

## Decoder

### Decoder Block(Layer)

```python
class DecoderBlock(nn.Module):
  def __init__(self, hidden_dim, n_heads, ff_dim, dropout_ratio, device):
    super().__init__()

    self.self_attention_layer = MultiHeadAttention(hidden_dim, n_heads, dropout_ratio, device)
    self.self_attention_norm = nn.LayerNorm(hidden_dim)
    self.encoder_attention = MultiHeadAttention(hidden_dim, n_heads, dropout_ratio, device)
    self.encoder_norm = nn.LayerNorm(hidden_dim)
    self.position_wise_ff_layer = PositionWiseFF(hidden_dim, ff_dim, dropout_ratio)
    self.position_wise_ff_norm = nn.LayerNorm(hidden_dim)

    self.dropout = nn.Dropout(dropout_ratio)

  def forward(self, tar, enc_src, tar_mask, src_mask):
    # trg: [batch_size, trg_len, hidden_dim]
    # enc_src: [batch_size, src_len, hidden_dim]
    # trg_mask: [batch_size, trg_len]
    # src_mask: [batch_size, src_len]
    
    # q,k,v = tar, í•„ìš”ì‹œ tar_mask ì‚¬ìš©
    _tar, attention = self.self_attention_layer(tar, tar, tar, tar_mask) # trg: [batch_size, trg_len, hidden_dim]
    tar = self.self_attention_norm(tar + self.dropout(_tar))

    # Encoder Attention
    # query : tar
    # key, value : enc_src (encoder output)
    _tar, attention = self.encoder_attention(tar, enc_src, enc_src, src_mask)
    tar = self.encoder_norm(tar + self.dropout(_tar))

    _tar = self.position_wise_ff_layer(tar)
    tar = self.position_wise_ff_norm(tar + self.dropout(_tar))

    return tar, attention # tar: [batch_size, src_len, hidden_dim], attention : [batch_size, n_heads, hidden_dim, src_len]
```

### Decoder

- Decoder Block ì´ n_layers ë§Œí¼ ì´ì–´ì§„ êµ¬ì¡°

```python
class Decoder(nn.Module):
  def __init__(self, output_dim, hidden_dim, n_heads, ff_dim, dropout_ratio, device, n_layers, max_len=100):
    super().__init__()

    self.device = device

    # embedding
    self.tok_embedding = nn.Embedding(output_dim, hidden_dim)
    self.pos_embedding = nn.Embedding(max_len, hidden_dim)

    self.layers = nn.ModuleList([DecoderBlock(hidden_dim, n_heads, ff_dim, dropout_ratio, device) for i in range(n_layers)])

    self.fc_out = nn.Linear(hidden_dim, output_dim)

    self.dropout = nn.Dropout(dropout_ratio)
    self.scale = torch.sqrt(torch.FloatTensor([hidden_dim])).to(device)

  def forward(self, tar, enc_src, tar_mask, src_mask):
    # trg: [batch_size, trg_len]
    # enc_src: [batch_size, src_len, hidden_dim]
    # trg_mask: [batch_size, trg_len]
    # src_mask: [batch_size, src_len]

    batch_size = tar.shape[0] # batch ë‚´ì˜ ë¬¸ì¥ ìˆ˜
    tar_len = tar.shape[1] # batch ë‚´ ë¬¸ì¥ ì¤‘ ê°€ì¥ ê¸´ ê²ƒì˜ ê¸¸ì´

    pos = torch.arange(0, tar_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)

    # ì†ŒìŠ¤ ë¬¸ì¥ì˜ ì„ë² ë”©ê³¼ ìœ„ì¹˜ ì„ë² ë”©ì„ ë”í•œ ê²ƒì„ ì‚¬ìš©
    tar = self.dropout((self.tok_embedding(tar) * self.scale) + self.pos_embedding(pos)) # tar: [batch_size, tar_len, hidden_dim]
    
    for layer in self.layers:
      tar, attention = layer(tar, enc_src, tar_mask, src_mask)

    out = self.fc_out(tar) # out: [batch_size, trg_len, output_dim]

    return out
```

## Transformer

```python
class Transformer(nn.Module):
  def __init__(self, encoder, decoder, src_pad_idx, tar_pad_idx, device):
    super().__init__()

    self.encoder = encoder
    self.decoder = decoder
    self.src_pad_idx = src_pad_idx
    self.tar_pad_idx = tar_pad_idx
    self.device = device

  def make_src_mask(self,src):
    # ì†ŒìŠ¤ ë¬¸ì¥ì˜ <pad> í† í°ì— ëŒ€í•˜ì—¬ ë§ˆìŠ¤í¬(mask) ê°’ì„ 0ìœ¼ë¡œ ì„¤ì •
    # src: [batch_size, src_len]

    src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2) # src_mask: [batch_size, 1, 1, src_len]

    return src_mask

  def make_tar_mask(self, tar):
    # íƒ€ê²Ÿ ë¬¸ì¥ì—ì„œ ê° ë‹¨ì–´ëŠ” ë‹¤ìŒ ë‹¨ì–´ê°€ ë¬´ì—‡ì¸ì§€ ì•Œ ìˆ˜ ì—†ë„ë¡(ì´ì „ ë‹¨ì–´ë§Œ ë³´ë„ë¡) ë§Œë“¤ê¸° ìœ„í•´ ë§ˆìŠ¤í¬ë¥¼ ì‚¬ìš©
    # tar: [batch_size, trg_len]

    tar_pad_mask = (tar != self.tar_pad_idx).unsqueeze(1).unsqueeze(2) # tar_pad_mask: [batch_size, 1, 1, trg_len]
    tar_len = tar.shape[1]
    tar_sub_mask = torch.tril(torch.ones((tar_len, tar_len), device = self.device)).bool() # tar_sub_mask: [tar_len, tar_len]

    tar_mask = tar_pad_mask & tar_sub_mask # tar_mask: [batch_size, 1, tar_len, tar_len]

    return tar_mask

  def forward(self, src, tar):
    # src: [batch_size, src_len]
    # trg: [batch_size, trg_len]

    src_mask = self.make_src_mask(src) # src_mask: [batch_size, 1, 1, src_len]
    tar_mask = self.make_tar_mask(tar) # tar_mask: [batch_size, 1, tar_len, tar_len]

    enc_src = self.encoder(src, src_mask)

    out = self.decoder(tar, enc_src, tar_mask, src_mask)

    return out
```

---

# Error Handling

### **SSL certificate error**

- Multi30k dataset ë¡œë“œ ê³¼ì •ì—ì„œ ë°œìƒ
- ssl certification ë¹„í™œì„±í™” ë° data ì§ì ‘ ë‹¤ìš´ë¡œë“œ, ê²½ë¡œ ìƒì„±

> **ref :** https://github.com/ndb796/Deep-Learning-Paper-Review-and-Practice/issues/12, 
        https://github.com/pytorch/pytorch/issues/33288#issuecomment-1086779194
> 

```python
# error message
[ssl: certificate_verify_failed] certificate verify failed: 
hostname mismatch, certificate is not valid for 'www.quest.dcs.shef.ac.uk'. (_ssl.c:1007)]
```

**í•´ê²°**

```python
# dataset load ì „ ì‹¤í–‰
import ssl
ssl._create_default_https_context = ssl._create_unverified_context
```

- [Dataset](https://github.com/zaidhassanch/PointerNetworks/tree/6ccd5ebad877c9fbc10ac3af10114b4a6097700b/data/multi30k) ë‹¤ìš´ë¡œë“œ ë° ê²½ë¡œ `data/multi30k` ì— íŒŒì¼ ë¡œë“œ

```python
# root='data' option ì¶”ê°€
train_dataset, valid_dataset, test_dataset 
	= Multi30k.splits(exts=('.de', '.en'), fields=(src, tgt), **root='data'**)
```

---

# Reference

- **torch.text - Field :** https://wikidocs.net/60314
- **spaCy :** https://ungodly-hour.tistory.com/37
- **BucketIterator :** https://torchtext.readthedocs.io/en/latest/data.html
- **Xavier Initialization :** [https://yngie-c.github.io/deep learning/2020/03/17/parameter_init/](https://yngie-c.github.io/deep%20learning/2020/03/17/parameter_init/)
- **Residual Connection :** [https://velog.io/@stapers/ë…¼ë¬¸-ìŠ¤í„°ë””-Week4-5-Attention-is-All-You-Need](https://velog.io/@stapers/%EB%85%BC%EB%AC%B8-%EC%8A%A4%ED%84%B0%EB%94%94-Week4-5-Attention-is-All-You-Need)
